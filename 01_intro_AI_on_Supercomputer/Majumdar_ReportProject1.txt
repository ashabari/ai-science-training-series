In this project, I explored the impact of varying batch sizes and learning rates on model convergence using gradient descent. I compared learning curves for different batch sizes, such as 32, 64, 128, 256, and 512, to understand their effects on training stability and performance. I observed that increasing the batch size and learning rate affects convergence, with higher values potentially leading to instability. We implemented mini-batch gradient descent, tracking loss values and slopes over iterations. The findings contribute to optimizing training processes for large datasets, demonstrating the balance needed between batch size and learning rate for effective convergence.
